# # src/models/model_manager.py
# import os
# import sys
# import yaml
# import shutil
# from typing import Dict, Optional, Any, List

# # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´
# def check_disk_space(path: str, required_gb: float) -> bool:
#     """æ£€æŸ¥æŒ‡å®šè·¯å¾„çš„ç£ç›˜ç©ºé—´"""
#     try:
#         total, used, free = shutil.disk_usage(path)
#         free_gb = free / (1024**3)  # è½¬æ¢ä¸ºGB
#         print(f"ğŸ“Š {path} å¯ç”¨ç©ºé—´: {free_gb:.1f}GB, éœ€è¦: {required_gb}GB")
#         return free_gb >= required_gb
#     except Exception as e:
#         print(f"âš ï¸ æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´: {e}")
#         return False  # å¦‚æœæ— æ³•æ£€æŸ¥ï¼Œå‡è®¾ç©ºé—´ä¸è¶³

# def get_best_cache_dir() -> str:
#     """è·å–æœ€ä½³ç¼“å­˜ç›®å½•"""
#     # å°è¯•çš„ç¼“å­˜è·¯å¾„ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
#     cache_options = [
#         ("D:/huggingface_cache", "Dç›˜ç¼“å­˜"),
#         ("E:/huggingface_cache", "Eç›˜ç¼“å­˜"),
#         ("C:/Users/Administrator/.cache/huggingface", "é»˜è®¤ç¼“å­˜"),
#         (os.path.expanduser("~/.cache/huggingface"), "ç”¨æˆ·ç¼“å­˜")
#     ]
    
#     for cache_path, description in cache_options:
#         try:
#             # åˆ›å»ºç›®å½•
#             os.makedirs(cache_path, exist_ok=True)
#             print(f"âœ… ä½¿ç”¨{description}: {cache_path}")
#             return cache_path
#         except Exception as e:
#             print(f"âŒ {description}ä¸å¯ç”¨: {e}")
    
#     # ä½¿ç”¨ä¸´æ—¶ç›®å½•ä½œä¸ºæœ€åçš„é€‰æ‹©
#     temp_dir = os.path.join(os.getcwd(), "temp_cache")
#     os.makedirs(temp_dir, exist_ok=True)
#     print(f"âš ï¸ ä½¿ç”¨ä¸´æ—¶ç¼“å­˜: {temp_dir}")
#     return temp_dir

# class ModelManager:
#     """ç®¡ç†å¤šä¸ªä»£ç LLMæ¨¡å‹"""
    
#     def __init__(self, config_path: str = "config.yaml"):
#         self.config = self._load_config(config_path)
#         self.models: Dict[str, Dict[str, Any]] = {}
#         self.current_model = None
        
#         # è®¾ç½®ç¼“å­˜è·¯å¾„åˆ°Dç›˜
#         self._setup_cache()
        
#         # æ ¹æ®ç£ç›˜ç©ºé—´æ™ºèƒ½é€‰æ‹©æ¨¡å‹
#         self._select_best_model()
    
#     def _setup_cache(self):
#         """è®¾ç½®ç¼“å­˜ç›®å½•"""
#         # è·å–æœ€ä½³ç¼“å­˜è·¯å¾„
#         self.cache_dir = get_best_cache_dir()
        
#         # è®¾ç½®ç¯å¢ƒå˜é‡
#         os.environ['HF_HOME'] = self.cache_dir
#         os.environ['TRANSFORMERS_CACHE'] = os.path.join(self.cache_dir, "models")
#         os.environ['HUGGINGFACE_HUB_CACHE'] = os.path.join(self.cache_dir, "hub")
#         os.environ['HF_DATASETS_CACHE'] = os.path.join(self.cache_dir, "datasets")
        
#         # è®¾ç½®å›½å†…é•œåƒï¼ˆåŠ é€Ÿä¸‹è½½ï¼‰
#         os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        
#         # åˆ›å»ºå­ç›®å½•
#         for subdir in ["models", "hub", "datasets"]:
#             path = os.path.join(self.cache_dir, subdir)
#             os.makedirs(path, exist_ok=True)
        
#         print(f"ğŸ“ æ¨¡å‹ç¼“å­˜ç›®å½•: {self.cache_dir}")
    
#     def _load_config(self, config_path: str) -> Dict[str, Any]:
#         """åŠ è½½é…ç½®"""
#         try:
#             with open(config_path, 'r', encoding='utf-8') as f:
#                 return yaml.safe_load(f)
#         except FileNotFoundError:
#             print(f"âš ï¸ é…ç½®æ–‡ä»¶ {config_path} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
#             return self._get_default_config()
#         except Exception as e:
#             print(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
#             return self._get_default_config()
    
#     def _get_default_config(self) -> Dict[str, Any]:
#         """è·å–é»˜è®¤é…ç½®"""
#         return {
#             'models': {
#                 'default': 'tiny-starcoder',
#                 'options': {
#                     'tiny-starcoder': {
#                         'name': 'bigcode/tiny_starcoder_py',
#                         'type': 'huggingface',
#                         'max_tokens': 512,
#                         'size_gb': 0.2
#                     },
#                     'simulated-model': {
#                         'name': 'simulated',
#                         'type': 'simulated',
#                         'max_tokens': 1024,
#                         'size_gb': 0
#                     }
#                 }
#             }
#         }
    
#     def _select_best_model(self):
#         """æ ¹æ®ç£ç›˜ç©ºé—´æ™ºèƒ½é€‰æ‹©æœ€ä½³æ¨¡å‹"""
#         if not self.config:
#             return
        
#         # è·å–æ‰€æœ‰æ¨¡å‹é€‰é¡¹
#         models = self.config.get('models', {}).get('options', {})
#         if not models:
#             return
        
#         # æ£€æŸ¥Dç›˜ç©ºé—´
#         cache_drive = os.path.splitdrive(self.cache_dir)[0]
#         if not cache_drive:
#             cache_drive = "C:"  # é»˜è®¤Cç›˜
        
#         available_models = []
        
#         for model_name, model_config in models.items():
#             required_gb = model_config.get('size_gb', 999)
            
#             # å¯¹äºæ¨¡æ‹Ÿæ¨¡å‹ï¼Œæ€»æ˜¯å¯ç”¨
#             if model_config.get('type') == 'simulated':
#                 available_models.append((model_name, model_config))
#                 continue
            
#             # æ£€æŸ¥ç£ç›˜ç©ºé—´
#             if check_disk_space(cache_drive, required_gb * 1.5):  # 1.5å€å®‰å…¨ç³»æ•°
#                 available_models.append((model_name, model_config))
#                 print(f"âœ… {model_name} å¯ç”¨ ({required_gb}GB)")
#             else:
#                 print(f"âŒ {model_name} ä¸å¯ç”¨ - ç£ç›˜ç©ºé—´ä¸è¶³")
        
#         # é€‰æ‹©æœ€å°çš„å¯ç”¨æ¨¡å‹
#         if available_models:
#             # æŒ‰å¤§å°æ’åº
#             available_models.sort(key=lambda x: x[1].get('size_gb', 999))
#             best_model = available_models[0][0]
            
#             # æ›´æ–°é»˜è®¤æ¨¡å‹
#             if 'models' in self.config and 'default' in self.config['models']:
#                 old_default = self.config['models']['default']
#                 self.config['models']['default'] = best_model
#                 print(f"ğŸ”„ è‡ªåŠ¨é€‰æ‹©æ¨¡å‹: {best_model} (åŸé»˜è®¤: {old_default})")
#         else:
#             print("âš ï¸ æ²¡æœ‰å¯ç”¨æ¨¡å‹ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
#             if 'models' in self.config:
#                 self.config['models']['default'] = 'simulated-model'
    
#     def get_model(self, model_name: Optional[str] = None) -> Dict[str, Any]:
#         """è·å–æ¨¡å‹ï¼Œå¸¦æœ‰è‡ªåŠ¨é™çº§åŠŸèƒ½"""
#         if not model_name:
#             model_name = self.config.get('models', {}).get('default', 'simulated-model')
        
#         # å¦‚æœå·²ç»åŠ è½½è¿‡ï¼Œç›´æ¥è¿”å›
#         if model_name in self.models:
#             return self.models[model_name]
        
#         # è·å–æ¨¡å‹é…ç½®
#         model_config = self.config.get('models', {}).get('options', {}).get(model_name)
#         if not model_config:
#             print(f"âŒ æ¨¡å‹ {model_name} é…ç½®ä¸å­˜åœ¨")
#             return self._get_simulated_model()
        
#         print(f"ğŸ¤– æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
#         print(f"   æ¨¡å‹å¤§å°: {model_config.get('size_gb', 'æœªçŸ¥')}GB")
#         print(f"   ç¼“å­˜ä½ç½®: {self.cache_dir}")
        
#         # å¦‚æœæ˜¯æ¨¡æ‹Ÿæ¨¡å‹ï¼Œç›´æ¥è¿”å›
#         if model_config.get('type') == 'simulated':
#             return self._get_simulated_model()
        
#         # å°è¯•åŠ è½½çœŸå®æ¨¡å‹
#         try:
#             return self._load_real_model(model_name, model_config)
#         except Exception as e:
#             print(f"âŒ åŠ è½½æ¨¡å‹ {model_name} å¤±è´¥: {e}")
            
#             # å°è¯•é™çº§åˆ°æ›´å°çš„æ¨¡å‹
#             return self._fallback_to_smaller_model(model_name)
    
#     def _load_real_model(self, model_name: str, model_config: Dict[str, Any]) -> Dict[str, Any]:
#         """åŠ è½½çœŸå®æ¨¡å‹"""
#         # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨æ²¡æœ‰å®‰è£…torchæ—¶å‡ºé”™
#         try:
#             import torch
#             from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
#         except ImportError as e:
#             print(f"âŒ ç¼ºå°‘ä¾èµ–: {e}")
#             raise
        
#         # æ£€æŸ¥ç£ç›˜ç©ºé—´
#         required_gb = model_config.get('size_gb', 999)
#         cache_drive = os.path.splitdrive(self.cache_dir)[0] or "C:"
        
#         if not check_disk_space(cache_drive, required_gb * 1.2):  # 20%é¢å¤–ç©ºé—´
#             raise OSError(f"ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œéœ€è¦{required_gb}GB")
        
#         # åŠ è½½tokenizer
#         tokenizer = AutoTokenizer.from_pretrained(
#             model_config['name'],
#             trust_remote_code=True,
#             padding_side="left",
#             cache_dir=os.path.join(self.cache_dir, "models")
#         )
        
#         # é…ç½®é‡åŒ–ï¼ˆèŠ‚çœå†…å­˜ï¼‰
#         quantization_config = None
#         if torch.cuda.is_available():
#             print("âœ… æ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨4-bité‡åŒ–")
#             quantization_config = BitsAndBytesConfig(
#                 load_in_4bit=True,
#                 bnb_4bit_compute_dtype=torch.float16,
#                 bnb_4bit_quant_type="nf4",
#                 bnb_4bit_use_double_quant=True,
#             )
        
#         # åŠ è½½æ¨¡å‹
#         model = AutoModelForCausalLM.from_pretrained(
#             model_config['name'],
#             quantization_config=quantization_config,
#             torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
#             device_map="auto" if torch.cuda.is_available() else None,
#             trust_remote_code=True,
#             use_safetensors=True,
#             cache_dir=os.path.join(self.cache_dir, "models")
#         )
        
#         # ç¡®ä¿pad_tokenè®¾ç½®
#         if tokenizer.pad_token is None:
#             tokenizer.pad_token = tokenizer.eos_token
        
#         model_info = {
#             'model': model,
#             'tokenizer': tokenizer,
#             'config': model_config,
#             'simulated': False
#         }
        
#         self.models[model_name] = model_info
#         self.current_model = model_name
        
#         print(f"âœ… æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
        
#         # æ‰“å°æ¨¡å‹ä¿¡æ¯
#         total_params = sum(p.numel() for p in model.parameters())
#         print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {total_params:,}")
#         if torch.cuda.is_available():
#             print(f"ğŸ® ä½¿ç”¨è®¾å¤‡: GPU ({torch.cuda.get_device_name(0)})")
#         else:
#             print("ğŸ’» ä½¿ç”¨è®¾å¤‡: CPU")
        
#         return model_info
    
#     def _fallback_to_smaller_model(self, failed_model: str) -> Dict[str, Any]:
#         """é™çº§åˆ°æ›´å°çš„æ¨¡å‹"""
#         print("ğŸ”„ å°è¯•é™çº§åˆ°æ›´å°çš„æ¨¡å‹...")
        
#         # è·å–æ‰€æœ‰æ¨¡å‹ï¼ŒæŒ‰å¤§å°æ’åº
#         all_models = list(self.config.get('models', {}).get('options', {}).items())
#         all_models.sort(key=lambda x: x[1].get('size_gb', 999))
        
#         # æ‰¾åˆ°å¤±è´¥çš„æ¨¡å‹ä½ç½®
#         failed_index = next((i for i, (name, _) in enumerate(all_models) 
#                            if name == failed_model), -1)
        
#         if failed_index == -1:
#             print("âŒ æ— æ³•æ‰¾åˆ°å¤±è´¥çš„æ¨¡å‹é…ç½®")
#             return self._get_simulated_model()
        
#         # å°è¯•æ›´å°çš„æ¨¡å‹
#         for i in range(failed_index + 1, len(all_models)):
#             model_name, model_config = all_models[i]
            
#             # è·³è¿‡æ¨¡æ‹Ÿæ¨¡å‹ï¼ˆæœ€åçš„é€‰æ‹©ï¼‰
#             if model_config.get('type') == 'simulated':
#                 continue
            
#             print(f"ğŸ”„ å°è¯•åŠ è½½: {model_name}")
#             try:
#                 return self._load_real_model(model_name, model_config)
#             except Exception as e:
#                 print(f"âŒ åŠ è½½ {model_name} å¤±è´¥: {e}")
#                 continue
        
#         # æ‰€æœ‰çœŸå®æ¨¡å‹éƒ½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹
#         print("âš ï¸ æ‰€æœ‰çœŸå®æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹")
#         return self._get_simulated_model()
    
#     def _get_simulated_model(self) -> Dict[str, Any]:
#         """è·å–æ¨¡æ‹Ÿæ¨¡å‹"""
#         print("ğŸ­ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹ï¼ˆæ— éœ€ä¸‹è½½ï¼‰")
        
#         model_info = {
#             'model': None,
#             'tokenizer': None,
#             'config': {
#                 'name': 'simulated-model',
#                 'type': 'simulated',
#                 'max_tokens': 1024,
#                 'size_gb': 0
#             },
#             'simulated': True
#         }
        
#         self.models['simulated-model'] = model_info
#         self.current_model = 'simulated-model'
        
#         return model_info
    
#     def list_available_models(self) -> List[str]:
#         """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"""
#         models = self.config.get('models', {}).get('options', {})
#         return list(models.keys())
    
#     def get_model_info(self, model_name: str) -> Dict[str, Any]:
#         """è·å–æ¨¡å‹ä¿¡æ¯"""
#         return self.config.get('models', {}).get('options', {}).get(model_name, {})

# src/models/model_manager.py
import os
import yaml
from typing import Dict, Optional, Any

class ModelManager:
    """æ¨¡å‹ç®¡ç†å™¨ - ä¸“é—¨ä½¿ç”¨å°æ¨¡å‹"""
    
    def __init__(self, config_path: str = "config.yaml"):
        # ç›´æ¥è®¾ç½®ç¼“å­˜åˆ°Dç›˜
        self._setup_disk_cache()
        
        # åŠ è½½é…ç½®
        self.config = self._load_config(config_path)
        
        # å¼ºåˆ¶ä½¿ç”¨å°æ¨¡å‹ï¼Œå¿½ç•¥é…ç½®æ–‡ä»¶
        self.config['models']['default'] = 'tiny-starcoder'
        
        self.models: Dict[str, Dict[str, Any]] = {}
        self.current_model = None
        
        print("ğŸ”§ å·²é…ç½®ä¸ºä½¿ç”¨å°æ¨¡å‹æ¨¡å¼")
    
    def _setup_disk_cache(self):
        """è®¾ç½®ç¼“å­˜åˆ°Dç›˜"""
        cache_dir = "D:/huggingface_cache_small"
        
        # åˆ›å»ºç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        os.makedirs(f"{cache_dir}/models", exist_ok=True)
        os.makedirs(f"{cache_dir}/hub", exist_ok=True)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['HF_HOME'] = cache_dir
        os.environ['TRANSFORMERS_CACHE'] = f"{cache_dir}/models"
        os.environ['HUGGINGFACE_HUB_CACHE'] = f"{cache_dir}/hub"
        
        # è®¾ç½®å›½å†…é•œåƒï¼ˆåŠ é€Ÿä¸‹è½½ï¼‰
        os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
        
        print(f"ğŸ“ ç¼“å­˜ç›®å½•: {cache_dir}")
        print("ğŸŒ ä½¿ç”¨å›½å†…é•œåƒåŠ é€Ÿ")
    
    def _load_config(self, config_path: str):
        """åŠ è½½é…ç½®ï¼Œä½†å¼ºåˆ¶ä½¿ç”¨å°æ¨¡å‹"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
        except:
            # å¦‚æœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
            config = self._get_default_config()
        
        # ç¡®ä¿é…ç½®ä¸­æœ‰tiny-starcoder
        if 'tiny-starcoder' not in config.get('models', {}).get('options', {}):
            config['models']['options']['tiny-starcoder'] = {
                'name': 'bigcode/tiny_starcoder_py',
                'type': 'huggingface',
                'max_tokens': 512,
                'size_gb': 0.2
            }
        
        return config
    
    def _get_default_config(self):
        """è·å–é»˜è®¤é…ç½®ï¼ˆå°æ¨¡å‹ï¼‰"""
        return {
            'models': {
                'default': 'tiny-starcoder',
                'options': {
                    'tiny-starcoder': {
                        'name': 'bigcode/tiny_starcoder_py',
                        'type': 'huggingface',
                        'max_tokens': 512,
                        'size_gb': 0.2
                    },
                    'simulated-model': {
                        'name': 'simulated',
                        'type': 'simulated',
                        'max_tokens': 1024,
                        'size_gb': 0
                    }
                }
            }
        }
    
    def get_model(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ - å¼ºåˆ¶ä½¿ç”¨å°æ¨¡å‹"""
        # å¼ºåˆ¶ä½¿ç”¨tiny-starcoder
        model_name = 'tiny-starcoder'
        
        if model_name in self.models:
            return self.models[model_name]
        
        model_config = self.config['models']['options'][model_name]
        
        print(f"ğŸš€ æ­£åœ¨åŠ è½½å°æ¨¡å‹: {model_name}")
        print(f"   æ¨¡å‹å¤§å°: {model_config['size_gb']}GB")
        print("   ä¸‹è½½å¾ˆå¿«ï¼Œè¯·ç¨å€™...")
        
        try:
            # å°è¯•å¯¼å…¥transformers
            import torch
            from transformers import AutoTokenizer, AutoModelForCausalLM
        except ImportError:
            print("âŒ æœªå®‰è£…transformersï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            return self._get_simulated_model()
        
        try:
            # åŠ è½½å°æ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(
                model_config['name'],
                trust_remote_code=True,
                padding_side="left"
            )
            
            model = AutoModelForCausalLM.from_pretrained(
                model_config['name'],
                torch_dtype=torch.float32,  # ä½¿ç”¨float32ï¼Œæ›´ç¨³å®š
                device_map="cpu",  # ä½¿ç”¨CPUï¼Œé¿å…GPUå†…å­˜é—®é¢˜
                trust_remote_code=True
            )
            
            # ç¡®ä¿pad_tokenè®¾ç½®
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model_info = {
                'model': model,
                'tokenizer': tokenizer,
                'config': model_config,
                'simulated': False
            }
            
            self.models[model_name] = model_info
            self.current_model = model_name
            
            print(f"âœ… å°æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
            
            return model_info
            
        except Exception as e:
            print(f"âŒ å°æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ”„ åˆ‡æ¢åˆ°æ¨¡æ‹Ÿæ¨¡å¼...")
            return self._get_simulated_model()
    
    def _get_simulated_model(self):
        """è·å–æ¨¡æ‹Ÿæ¨¡å‹"""
        print("ğŸ­ ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å‹ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰")
        
        model_info = {
            'model': None,
            'tokenizer': None,
            'config': {
                'name': 'simulated-model',
                'type': 'simulated',
                'max_tokens': 1024,
                'size_gb': 0
            },
            'simulated': True
        }
        
        self.models['simulated-model'] = model_info
        self.current_model = 'simulated-model'
        
        return model_info